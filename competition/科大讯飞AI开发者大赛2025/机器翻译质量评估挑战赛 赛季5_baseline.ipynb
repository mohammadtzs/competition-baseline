{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d43e6f2-d352-415e-a6fa-8ff13dd3fbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc7d8997d9174aad8cac20193f2ca100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A large language model (LLM) is a type of artificial intelligence designed to understand and generate human-like text based on vast amounts of training data. These models are trained using advanced machine learning techniques, often involving deep neural networks, to recognize patterns in language. \\n\\nKey characteristics of LLMs include their ability to handle complex tasks such as translation, summarization, question-answering, and even creative writing. They can process and produce text in multiple languages, making them valuable tools for language translation and content generation. LLMs continue to evolve with advancements in natural language processing, aiming to improve their accuracy, efficiency, and adaptability to diverse linguistic contexts.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/home/lyz/hf-models/Qwen2.5-3B-Instruct/\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/lyz/hf-models/Qwen2.5-3B-Instruct/\")\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94287f31-a1f4-4077-a328-f3225ae43ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e595f5be-6e65-406e-a9f4-a0016ad686be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8692142  0.8665447 ]\n",
      " [0.89948714 0.87843263]]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentences_1 = [\"样例数据-1\", \"样例数据-2\"]\n",
    "sentences_2 = [\"样例数据-3\", \"样例数据-4\"]\n",
    "bge_model = SentenceTransformer('/home/lyz/hf-models/bge-small-zh-v1.5/')\n",
    "embeddings_1 = bge_model.encode(sentences_1, normalize_embeddings=True)\n",
    "embeddings_2 = bge_model.encode(sentences_2, normalize_embeddings=True)\n",
    "similarity = embeddings_1 @ embeddings_2.T\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2df95120-a710-4441-975e-554579e6413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('test/test_cn2en.txt', sep='\\t', header=None)\n",
    "for row in data.iterrows():\n",
    "    if row[1][0][0].lower() in string.ascii_letters:\n",
    "        s1 = row[1][0]\n",
    "        s2 = row[1][1]\n",
    "    else:\n",
    "        s1 = row[1][1]\n",
    "        s2 = row[1][0]\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"将中文翻译为英文，不要有其他输出：{s1}\"},\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    embeddings_1 = bge_model.encode([response, s2], normalize_embeddings=True)\n",
    "    score = np.dot(embeddings_1[0], embeddings_1[1])\n",
    "    score = int(score * 100)\n",
    "\n",
    "    with open('submit/cn2en.txt', 'a') as up:\n",
    "        up.write(f'{score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cc48c08-54d5-4576-9e03-212f86d4ebc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('test/test_en2cn.txt', sep='\\t', header=None)\n",
    "for row in data.iterrows():\n",
    "    if row[1][0][0].lower() in string.ascii_letters:\n",
    "        s1 = row[1][0]\n",
    "        s2 = row[1][1]\n",
    "    else:\n",
    "        s1 = row[1][1]\n",
    "        s2 = row[1][0]\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"将英文翻译为中文，不要有其他输出：{s1}\"},\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    embeddings_1 = bge_model.encode([response, s2], normalize_embeddings=True)\n",
    "    score = np.dot(embeddings_1[0], embeddings_1[1])\n",
    "    score = int(score * 100)\n",
    "\n",
    "    with open('submit/en2cn.txt', 'a') as up:\n",
    "        up.write(f'{score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "600edd79-81ea-48b3-956a-69e21eee572a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: 无法创建目录 \"submit\": 文件已存在\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: submit/ (stored 0%)\n",
      "  adding: submit/cn2en.txt (deflated 65%)\n",
      "  adding: submit/en2cn.txt (deflated 63%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!mkdir submit\n",
    "!\\rm submit/.ipynb_checkpoints/ -rf\n",
    "!zip -r submit.zip submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02aa56cd-95ab-41ad-9382-1da8c0ff85a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.11",
   "language": "python",
   "name": "py3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
